---
title: "Student's Performance"
author: "Promi Roy"
date: '2022-09-16'
output:
  html_document: default
  pdf_document: default
---
## Introduction to Data
#### Data Source - https://www.kaggle.com/code/scubethoven/student-grade-prediction/data

#### Attribute Information:

1. school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)

2. sex - student's sex (binary: 'F' - female or 'M' - male)

3. age - student's age (numeric: from 15 to 22)

4. address - student's home address type (binary: 'U' - urban or 'R' - rural)

5. famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)

6. Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)

7. Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - primary education (5th grade to 9th grade), 3 - secondary education or 4 - higher education)

8. Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - primary education (5th to 9th grade), 3 - secondary education or 4 - higher education)

9. Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')

10. Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')

11. reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')

12. guardian - student's guardian (nominal: 'mother', 'father' or 'other')

13. traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)

14. studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)

15. failures - number of past class failures (numeric: n if 1<=n<3, else 4)

16. schoolsup - extra educational support (binary: yes or no)

17. famsup - family educational support (binary: yes or no)

18. paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)

19. activities - extra-curricular activities (binary: yes or no)

20. nursery - attended nursery school (binary: yes or no)

21. higher - wants to take higher education (binary: yes or no)

22. internet - Internet access at home (binary: yes or no)

23. romantic - with a romantic relationship (binary: yes or no)

24. famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)

25. freetime - free time after school (numeric: from 1 - very low to 5 - very high)

26. goout - going out with friends (numeric: from 1 - very low to 5 - very high)

27. Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)

28. Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)

29. health - current health status (numeric: from 1 - very bad to 5 - very good)

30. absences - number of school absences (numeric: from 0 to 93)

31. G1 - 1st period grade

32. G2 - 2nd period grade

33. G3 - Final grade (Target variable)


```{r setup, include=FALSE}
library(tidyr)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(car)
install.packages("https://cran.r-project.org/src/contrib/Archive/rlang/rlang_1.0.4.tar.gz", repo=NULL, type="source")
library(rlang)
library(caret)
library(caTools)
library(carData)
library(xgboost)
library(miscTools)
library(hrbrthemes)
library(corrplot)
```

## Exploratory Data Analysis

```{r}
#Loading data
df<-read.csv("C:/Users/Promi/OneDrive/Desktop/student-mat.csv", header=T)
df=data.frame(df)
attach(df)
head(df)
```
 
```{r}
# Summary of the data
summary(df)

# Checking null values
is.null(df) 
```
There are no null values in the dataset.
```{r}
#Checking unique values for eah attribute
list_unique<-lapply(df, unique)
list_unique
```

```{r}
#Converting categorical variables to factor
names <- c(1:2,4:12,16:29)
df[,names] <- lapply(df[,names] , factor)
str(df)
```


```{r}
plots <- list()
p_names = c("Medu","Fedu","famrel","freetime","goout","Dalc","Walc","health","school","sex","address","famsize","Pstatus","Mjob","Fjob","reason","guardian","schoolsup","famsup","paid","activities","nursery","higher","internet","romantic")
for(nm in p_names) {
  plots[[nm]] <- ggplot(data= df,aes_string(x =nm,y=G3,fill=nm))+geom_bar(stat = "identity")
  print(plots[[nm]])
}
```

From the bar plots we can see more students are interested to have higher education. Before doing any test we would create some contingency table to check what motivates the students to have higher education. In other words, what other variables are associated with higher education.

We will do a chi Square test to check if other categorical variables are significantly associated with interest in higher education. 

Chi-square test can helps us to find out whether a difference between two categorical variables is due to chance or a relationship between them.

Let's see if mother's education has anything to do with student's interest in higher education. Let's create a barplot for both.
```{r}
#Barplot w.r.t mother's education and interest in higher studies
MeduVshigher=table(higher,Medu) #Contingency table
barplot(MeduVshigher,legend.text = TRUE, beside=TRUE, args.legend = list(x = "topright",
                           inset = c(- 0.05, 0)))

```
 
 The bar plot clearly shows the more educated the mother is the more children are interested in higher education.
```{r}
#Chisquare test w.r.t mother's education and interest in higher studies
chisq.test(df$Medu, df$higher)
```

Here the p value is less than 0.05 which concludes there is a significant relationship between mother's education and interest in higher studies.Now, we will try for other categorical variables, if there are significant association between interest in higher education and other categorical variables.

Let us create contingency tables for higher education with other categorical varables.
```{r}
#Contingency table
#i=c(1:2,4:12,16:29)
df_list_f <- function(x) (table(higher, x))
i=c(1:2,4:12,16:20,22:29)
df2 <- df[,i] 
lapply(df2, df_list_f)
```

The contingency tables gives us a potentiality of having association between interest in higher education and other categorical variables.We need to deep dive to check if there is significant association between interest in higher education with other categorical variables.

```{r}
#Contingency table
df_list_f <- function(x) chisq.test(table(higher, x))
i=c(1:2,4:12,16:20,22:29)
df2 <- df[,i] # df2 contains the columns vs, am, gear and carb
lapply(df2, df_list_f)


```

From the chisquare tests which has p value less than 0.05 we have found that sex, mother's education, father's education, reason for choosing school, extra payment for classes, going out, consuming alcohol on workdays and consuming alcohol on weekends are significantly associated with higher education.

 
```{r}
#Chisquare test
df_list_f <- function(x) chisq.test(table(goout, x))
i=c(19,23,25,27:28)
df2 <- df[,i] 
lapply(df2, df_list_f)


```

Wee have checked the association between going out and taking alcohol on working days, alcohol on weekends, involving in a romantic relationship, when they have free time and doing activities.I found that taking alcohol on working days, alcohol on weekends, and when they have free time are significantly associated with going out.

Now we will create more bar plots with respect to interest in higher education to see any pattern.
```{r}

ggplot(df) +
  geom_bar(aes(x=Dalc, fill=goout),
           position = "dodge") +
  facet_wrap(~higher) #Interest in higher education, going out with friends, taking alcohol on workdays

ggplot(df) +
  geom_bar(aes(x=Walc, fill=goout),
           position = "dodge") +
  facet_wrap(~higher) #Interest in higher education, going out with friends, taking alcohol on weekends

ggplot(df) +
  geom_bar(aes(x=activities, fill=goout),
           position = "dodge") +
  facet_wrap(~higher)  #Interest in higher education, going out with friends, involved in activities

ggplot(df) +
  geom_bar(aes(x=paid, fill=Mjob),
           position = "dodge") +
  facet_wrap(~higher)  #Interest in higher education, mother's job, paid for studies


p_grph <- ggplot(df) +
  geom_bar(aes(x=Fjob, fill=paid),
           position = "dodge") +
  facet_wrap(~higher)  #Interest in higher education, father's job, paid for studies


```

Now it's time to check correlation between continuous variables.We have found that G1 and G2 is highly correlated with G3.
```{r}
library(tidyverse)
dat <- df %>%
  select(age, traveltime, studytime,failures,absences,G1, G2, G3)
cor(dat)
#pairs.panel(dat,col="red")
```

```{r}
#Checking correlation of continuous variables graphically
library(corrplot)
M<-cor(dat)
corrplot(M, method="color")
```
From the above correlation matrix we found that the 1st grade "G1" and the 2nd grade "G2" is correlated and the value is 0.85. As it is more than 70%, we will add G1 and G2.


```{r}
df$G <- df$G1+df$G2
df$G
df = subset(df, select = -c(G1,G2) )

```



```{r}
#Creating histogram of continuous variables
par(mfrow = c(2, 2))  # Set up a 2 x 2 plotting space

# Create the loop.vector (all the columns)
loop.vector <- c("age","traveltime","studytime","failures","absences","G","G3")

for (i in loop.vector) { # Loop over loop.vector

  # store data in column.i as x
  x <- df[,i]
  
  # Plot histogram of x
  hist(x,
       main = paste(i),
       xlim = c(0, 30))
}

```


## Predictive Data Analysis

### Multiple Linear Regression

Before doing multiple linear regression, we usually are interested in answering a few important questions.
1. Is at least one of the predictors $X_1$, $X_2$, ..., $X_p$ useful in predicting the response?

2. Do all the predictors help to explain Y, or is only subset of the predictors useful?

3. How well does the model fit the data?

4. Given a set of predictor values, what response would should we predict.



Before answering these questions we will first check

1. Non-linearity of the response-predictor relationship

2. Correlation of error terms

3. Non-constant variance of error terms

4. Outliers

5. High-leverage points

6. Colinearity



```{r}
#Ploting the model
model=lm(G3~., data=df)
par(mfrow = c(2, 2))
plot(model)

```

```{r}
#Checking Multicolinearity
#create vector of VIF values
vif_values <- vif(model)
vif_values

```
From the above image of Normal Q-Q plot we can say the data follows normal distribution.

The image above shows the “Residual vs. Fitted”-plot and the “Scale-Location”-plot for a regression model without heteroscedastic residuals. In other words, the variance of the residuals is the same for all values of the fitted values. 

The residual plot is not showing any trend, just some outliers.So we can say there is no correlation among the errors.

From the residual vs. leverage plot we don't see any high leverage points. 


Now we will start answering our questions that we have set above. The first question we need to ask whether all of the regression coefficients are zero. So, we test the null hypothesis as

$H_0$: All the regression coefficients are zero.

$H_1$: Atleast one regression coefficient is non-zero.

The hypothesis test is performed by computing the F-statistic.

Before starting the process the data is splitted into train and test data. We would build the model based on the training data.
```{r}
#Splitting the data into train and test set
set.seed(0)
parts = createDataPartition(df$G3, p = .8, list = F)
train = df[parts, ]
test = df[-parts, ]

```

```{r}
#Creating model including all variables
fullmodel=lm(G3~., data=train)
summary(fullmodel)
```

When there is no relationship between the response and predictors, one would expect to take on a value close to 1. Here our F statistic is 24.38. Since this is larger than 1, it provides compelling evidence against null hypothesis.Also, the p value associated with the F statistic is essentially zero, so we have extremely strong evidence that at least one of the predictors is useful predicting the response variable.This answers our first question.


It is possible that all the predictors are associated with the response but it is more often the case that the response is only associated with a subset of predictors.The task of determining which predictors are associated with response, in order to fit a single model involving only those predictors is referred to as variable selection.

There are automated and efficient classical approaches to choose a smaller set of models to consider. These are forward selection, backward selection, and mixed or stepwise selection. The stepwise selection is a combination of both forward and backward selection. It stats with no variable. Then adds variables one by one. And, if at any point the p value of a variable rises above a certain threshold it was then dropped from the model.


```{r}
#define intercept-only model
intercept_only <- lm(G3 ~ 1, data=train)

#define model with all predictors
all <- lm(G3 ~ ., data=train)

#Reduced model
#perform backward stepwise regression
both <- step(intercept_only, direction='both', scope=formula(all), trace=0)
#view results of backward stepwise regression
both

```


```{r}

summary(both)

```



 Thus we get our selected predictors that explain our response variable. These are 1st grade, absences from school, family relationship, study time, 2nd grade, activities, school, reason for choosing the school and interest in higher education. 

There are different approaches to judge the model fitting. These include Mallow's Cp, Akaike Information Criterion(AIC), Bayesian Information Criterion(BIC), adjusted $R^2$.

To check how well the model fitted the data we have check $R^2$ and RSE, two common numerical measures of model fit.Here the $R^2$ value is close to 1, which means 86% variation in the response variable can be explained by the model.The model hat includes all the predictors has a small increase in $R^2$ compared to our reduced model.Additionally, The model has the lowest AIC value.However, it turns out that the model has some insignificant predictors. 

Also, the full model has RSE 1.88, However , the reduced model is slightly lesser than that.

Interpretation of beta coefficients:

On avereage for 1 score increase in 2nd grade their final grade increases by 0.97 points while keeping all other variables constant.

On avereage for 1 day increase in absence their final grade increases by 0.05 points while keeping all other variables constant.

On avereage for 1 hr increase in studytime their final grade decreases by 0.32 points while keeping all other variables constant.

On avereage for 1 hr increase in age their final grade decreases by 0.28 points while keeping all other variables constant.

On avereage for 1 hr increase in 1st grade G1 their final grade increases by 0.17 points while keeping all other variables constant.

On average students who do activities gets 0.58 less points in final grade than their reference group.

On average students who are interested in higher education gets 0.86 more points than their reference group.


Then comes prediction.The prediction has been done in test data and then created a scatter plot with predicted G3 and original G3. 



```{r}
predicted<-predict(both, test)
test["predicted"]<-predicted
plot(test$predicted, test$G3, xlab="G3", ylab="Predicted")
abline(lm(test$predicted~test$G3))
```
 
Then we calculated $R^2$ on test data. The $R^2$ in test data is little bit less than $R^2$ in training data.But still is good enough explain the variation of our data.
 
```{r}
require(miscTools)
r2 <- rSquared(test$G3, resid = test$G3-test$predicted)
r2
```

Stepwise process are not always good when accuracy of the model comes into question.We have some variables that are insignificant. Dropping those variables might hurt our $R^2$. However it is easy to use as it is automated. So, we will go for another approach which is known as K-fold cross validation.

### K-Fold Cross Validation


To evaluate the performance of a model on a dataset, we need to measure how well the predictions made by the model match the observed data.

One commonly used method for doing this is known as k-fold cross-validation, which uses the following approach:

1. Randomly divide a dataset into k groups, or “folds”, of roughly equal size.

2. Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds. Calculate the test MSE on the observations in the fold that was held out.

3. Repeat this process k times, using a different set each time as the holdout set.

4. Calculate the overall test MSE to be the average of the k test MSE’s.

The following code shows how to fit a multiple linear regression model to this dataset in R and perform k-fold cross validation with k = 10 folds to evaluate the model performance:

```{r}
#specify the cross-validation method
ctrl <- trainControl(method = "cv", number = 5)

#fit a regression model and use k-fold CV to evaluate performance
model1 <- train(G3 ~ ., data = df, method = "lm", trControl = ctrl)

#view summary of k-fold CV               
print(model1)
```


Interpretation of the output:

•	No pre-processing occurred. That is, we didn’t scale the data in any way before fitting the models.

•	The resampling method we used to evaluate the model was cross-validation with 5 folds.

•	The sample size for each training set was 315 to 316.

•	RMSE: The root mean squared error. This measures the average difference between the predictions made by the model and the actual observations. The lower the RMSE, the more closely a model can predict the actual observations.

•	R squared: This is a measure of the correlation between the predictions made by the model and the actual observations. The higher the R-squared, the more closely a model can predict the actual observations.

•	MAE: The mean absolute error. This is the average absolute difference between the predictions made by the model and the actual observations. The lower the MAE, the more closely a model can predict the actual observations.



```{r}
#view final model
model1$finalModel
```

The final model turns out to be: G3 = 1.88923 -0.02918*(PstatusT) – 1.12538(x2)
```{r}
#view predictions for each fold
model1$resample
```


Advantages of K-fold Cross-Validation

•	Fast computation speed.

•	A very effective method to estimate the prediction error and the accuracy of a model.
Disadvantages of K-fold Cross-Validation

•	A lower value of K leads to a biased model and a higher value of K can lead to variability in the performance
metrics of the model. Thus, it is very important to use the correct value of K for the model (generally K = 5 and K = 10 is desirable).



###Ridge & Lasso Regression
```{r}
#define response variable
y <- df$G3

#define matrix of predictor variables
x <- data.matrix(df[, c(1:30, 32)])
```

```{r}
library(glmnet)

#fit ridge regression model
model <- glmnet(x, y, alpha = 0)

#view summary of model
summary(model)
```

```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 0)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda

```

```{r}

#produce plot of test MSE by lambda value
plot(cv_model) 
```

```{r}
#find coefficients of best model
best_model <- glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(best_model)
```

```{r}
#produce Ridge trace plot
plot(model, xvar = "lambda")
```

```{r}
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
```

```{r}
#define response variable
y <- df$G3

#define matrix of predictor variables
x <- data.matrix(df[, c(1:30, 32)])
```

```{r}
library(glmnet)

#fit ridge regression model
model <- glmnet(x, y, alpha = 1)

#view summary of model
summary(model)
```

```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda

```

```{r}

#produce plot of test MSE by lambda value
plot(cv_model) 
```

```{r}
#find coefficients of best model
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
```

```{r}
#produce Ridge trace plot
plot(model, xvar = "lambda")
```

```{r}
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = x)
#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
```
